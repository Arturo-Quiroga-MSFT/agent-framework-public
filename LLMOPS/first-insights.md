
# LLMOps for Microsoft Agent Framework (MAF)

> **Status**: âœ… Complete implementation with documentation, code modules, and examples  
> **Location**: `AQ-CODE/llmops/`  
> **Last Updated**: November 3, 2025

---

## ðŸ“š Documentation Created

1. **[MAF_LLMOPS_BEST_PRACTICES.md](../AQ-CODE/llmops/MAF_LLMOPS_BEST_PRACTICES.md)** (60+ pages)
   - Comprehensive guide covering all aspects of LLMOps for MAF
   - Development, testing, deployment, monitoring, governance
   - Real-world implementation examples
   - Azure services integration

2. **[QUICK_REFERENCE.md](../AQ-CODE/llmops/QUICK_REFERENCE.md)**
   - Cheat sheet for quick implementation
   - Common patterns and code snippets
   - Troubleshooting guide
   - Customer conversation starters

3. **[README.md](../AQ-CODE/llmops/README.md)**
   - Module documentation
   - Quick start guide
   - Usage examples
   - Monitoring queries

---

## ðŸ”§ LLMOps Modules Implemented

All modules are in `AQ-CODE/llmops/`:

### 1. `observability.py`
- Application Insights integration
- OpenTelemetry tracing
- Custom metrics (agent calls, latency, tokens)
- Span creation for detailed tracking

### 2. `cost_tracker.py`
- Real-time cost calculation
- Token usage tracking per agent
- Model pricing database
- Cost aggregation and reporting

### 3. `budget_manager.py`
- Daily token budget enforcement
- Per-request token limits
- Automatic daily reset
- Usage statistics

### 4. `evaluator.py`
- Response quality scoring
- Topic coverage analysis
- Citation detection
- Structure and sentiment checking
- Quality labels (Excellent/Good/Fair/Poor)

---

## ðŸš€ Working Example

**File**: `AQ-CODE/llmops/example_production_agent.py`

A complete production-ready MAF agent demonstrating:
- Budget checking before requests
- Cost tracking with real-time calculations
- Response evaluation with metrics
- Observability tracking
- Comprehensive logging

**Run it:**
```bash
python AQ-CODE/llmops/example_production_agent.py
```

---

## ðŸ’¡ Key Insights

## **Why LLMOps Matters for MAF**

### **1. Agent Lifecycle Management**
When you build agents with MAF, you're creating production systems that need:
- **Version control** for agent configurations, prompts, and tools
- **Testing & validation** of agent behavior across scenarios
- **Monitoring** agent performance and quality
- **Deployment pipelines** for updates and rollbacks

### **2. MAF Already Has LLMOps Built-In**

MAF includes observability features you've been using:

```python
from agent_framework.observability import setup_observability

# Tracing options you've used:
setup_observability(
    enable_sensitive_data=True,
    applicationinsights_connection_string=app_insights_conn_str  # LLMOps monitoring
)
```

This gives you:
- **Trace logging** - Every agent interaction, tool call, LLM request
- **Application Insights integration** - Metrics, logs, dashboards
- **OTLP support** - Integration with observability platforms

### **3. LLMOps Concerns in MAF Projects**

**Prompt Management:**
- Agent instructions are prompts that need versioning
- Example from your stock bubble workflow:
```python
market_analyst = agent_client.create_agent(
    instructions=(
        "You're a senior market analyst..."  # This prompt needs LLMOps
    ),
    name="market_analyst"
)
```

**Evaluation & Testing:**
- Testing multi-agent workflows
- Evaluating response quality
- Regression testing when updating models

**Model Management:**
- Switching between models (gpt-4.1, gpt-4o, gpt-5-mini)
- A/B testing different models
- Cost optimization

**Deployment:**
- Your Docker + Azure Container Apps setup IS LLMOps
- Environment management (.env files, secrets)
- CI/CD for agent applications

### **4. Azure AI Foundry = LLMOps Platform for MAF**

Azure AI Foundry provides:
- **Prompt flow** - Visual workflow design & testing
- **Evaluation tools** - Built-in metrics for agents
- **Model catalog** - Centralized model management
- **Monitoring** - App Insights integration
- **Content safety** - Filters and guardrails

## **LLMOps Conversation Angles for MAF**

### **For Customers Building Agents:**
1. **"How do you monitor agent quality in production?"**
   - MAF â†’ App Insights tracing
   - Azure AI Foundry evaluation metrics

2. **"How do you manage prompt versions across teams?"**
   - Git + agent configuration files
   - Azure AI Foundry prompt flow

3. **"How do you test multi-agent workflows?"**
   - MAF DevUI for manual testing
   - Automated eval frameworks (your workflow outputs can be tested)

4. **"How do you handle model updates?"**
   - Environment variables for model switching
   - Gradual rollout with A/B testing

### **For Enterprise Adoption:**
- **Governance**: Version control, audit trails (tracing)
- **Compliance**: Content filtering, data privacy
- **Cost control**: Token usage monitoring, model optimization
- **Reliability**: Health checks, failover, retry logic

## **Bottom Line**

**Yes, absolutely bring LLMOps into MAF conversations!** 

MAF is the **framework** for building agents, and **LLMOps is the discipline** for operationalizing them. They're complementary:

- **MAF** = Build agents (code, workflows, tools)
- **LLMOps** = Operate agents (deploy, monitor, improve)

Your current setup already demonstrates LLMOps:
- Tracing with App Insights âœ…
- Docker containerization âœ…
- Azure Container Apps deployment âœ…
- Environment-based configuration âœ…
- Managed Identity auth âœ…

