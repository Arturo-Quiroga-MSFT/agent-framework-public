# Copyright (c) Microsoft. All rights reserved.
"""
Custom Executors for NL2SQL Pipeline

This module contains business logic executors that handle:
- Schema retrieval from MSSQL database
- SQL query validation and safety checks
- Query execution with error handling
- Result formatting and pagination
"""
import logging
import os
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any

from agent_framework import ChatMessage, Executor, Role, WorkflowContext, handler
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)


# ============================================================================
# Data Models
# ============================================================================


class UserQuestion(BaseModel):
    """User's natural language question."""

    question: str = Field(..., description="The user's natural language question about the data")


@dataclass
class SchemaContext:
    """Database schema information for SQL generation."""

    tables: list[dict[str, Any]]
    schemas: list[str]
    connection_id: str
    database_name: str


@dataclass
class GeneratedSQL:
    """SQL query generated by the LLM."""

    query: str
    explanation: str
    confidence: float = 1.0


@dataclass
class ValidatedSQL:
    """Validated and safe SQL query."""

    query: str
    is_safe: bool
    warnings: list[str]
    estimated_rows: int | None = None


@dataclass
class QueryResults:
    """Results from SQL query execution."""

    success: bool
    rows: list[dict[str, Any]] | None = None
    row_count: int = 0
    columns: list[str] | None = None
    error: str | None = None
    execution_time_ms: float = 0


# ============================================================================
# Input Normalizer Executor
# ============================================================================


class InputNormalizerExecutor(Executor):
    """Normalizes user input into structured format for the pipeline."""

    def __init__(self, id: str = "input_normalizer"):
        super().__init__(id=id)

    @handler
    async def normalize_string(self, question: str, ctx: WorkflowContext[list[ChatMessage]]) -> None:
        """Handle raw string input."""
        logger.info(f"Normalizing question: {question}")
        message = ChatMessage(
            role=Role.USER,
            text=f"User Question: {question}\n\nPlease help me query the database to answer this question.",
        )
        await ctx.send_message([message])

    @handler
    async def normalize_model(self, question: UserQuestion, ctx: WorkflowContext[list[ChatMessage]]) -> None:
        """Handle structured UserQuestion input."""
        logger.info(f"Normalizing structured question: {question.question}")
        message = ChatMessage(
            role=Role.USER,
            text=f"User Question: {question.question}\n\nPlease help me query the database to answer this question.",
        )
        await ctx.send_message([message])


# ============================================================================
# Schema Retriever Executor
# ============================================================================


class SchemaRetrieverExecutor(Executor):
    """Retrieves database schema information from Azure SQL Database."""

    def __init__(self, use_real_db: bool = True, id: str = "schema_retriever"):
        super().__init__(id=id)
        self.use_real_db = use_real_db

    @handler
    async def retrieve_schema(self, messages: list[ChatMessage], ctx: WorkflowContext[list[ChatMessage]]) -> None:
        """Retrieve schema and send as context to SQL generator.
        
        If use_real_db=True, connects to actual database and retrieves real schema.
        Otherwise, returns mock schema for demo purposes.
        """
        logger.info(f"Retrieving schema (real_db={self.use_real_db})")

        try:
            if self.use_real_db:
                schema_text = await self._get_real_schema()
            else:
                schema_text = self._get_mock_schema()
                logger.warning("âš ï¸  Using mock schema - set use_real_db=True for actual schema")

            # Get the original question from the last message
            original_text = messages[-1].text if messages else "No question provided"

            # Create message with schema context
            enhanced_message = ChatMessage(
                role=Role.SYSTEM,
                text=f"""DATABASE SCHEMA CONTEXT:

{schema_text}

ORIGINAL USER QUESTION:
{original_text}

Your task: Generate a SQL query to answer this question using the provided schema.""",
            )

            await ctx.send_message([enhanced_message])

        except Exception as e:
            logger.error(f"Schema retrieval failed: {e}")
            error_message = ChatMessage(
                role=Role.SYSTEM,
                text=f"Error retrieving schema: {str(e)}. Please check database connection.",
            )
            await ctx.send_message([error_message])

    async def _get_real_schema(self) -> str:
        """Retrieve real schema from the database.
        
        Returns:
            Formatted schema text for LLM consumption
        """
        from db_utils import create_connection_from_env
        
        logger.info("Connecting to database to retrieve schema...")
        
        with create_connection_from_env() as db:
            schema_info = db.get_schema_info()
        
        # Format schema for LLM
        lines = [
            f"Database: {db.database}",
            f"Server: {db.server}",
            "",
            "Available Tables:",
        ]
        
        for table_name, table_info in schema_info.items():
            schema_name = table_info["schema"]
            table = table_info["table"]
            columns = table_info["columns"]
            
            lines.append(f"\n  Schema: {schema_name}")
            lines.append(f"  Table: {table}")
            lines.append(f"  Columns:")
            
            for col in columns:
                col_def = f"    - {col['name']} ({col['data_type']}"
                
                if 'max_length' in col and col['max_length']:
                    col_def += f", length={col['max_length']}"
                elif 'precision' in col and col['precision']:
                    col_def += f", precision={col['precision']}"
                    if 'scale' in col and col['scale']:
                        col_def += f",{col['scale']}"
                
                if not col['nullable']:
                    col_def += ", NOT NULL"
                
                col_def += ")"
                lines.append(col_def)
        
        logger.info(f"âœ… Retrieved schema for {len(schema_info)} tables from database")
        return "\n".join(lines)

    def _get_mock_schema(self) -> str:
        """Return mock schema for demonstration purposes.
        
        TODO: Replace with actual schema retrieval from database.
        """
        return """Database: SampleDB
Server: Azure SQL Database

Available Schemas:
  - dbo

Available Tables:
  Schema: dbo
    - Customers (CustomerID INT, CustomerName NVARCHAR(100), Email NVARCHAR(100), Region NVARCHAR(50))
    - Orders (OrderID INT, CustomerID INT, OrderDate DATETIME, TotalAmount DECIMAL(18,2))
    - Products (ProductID INT, ProductName NVARCHAR(100), Category NVARCHAR(50), Price DECIMAL(18,2), StockQuantity INT)
    - OrderDetails (OrderDetailID INT, OrderID INT, ProductID INT, Quantity INT, UnitPrice DECIMAL(18,2))

Note: This is a mock schema. Implement proper schema retrieval for your database."""

    def _format_schema_context(self, conn_details: dict, schemas: list, tables: list) -> str:
        """Format schema information for LLM consumption."""
        lines = [
            f"Database: {conn_details.get('database', 'N/A')}",
            f"Server: {conn_details.get('server', 'N/A')}",
            "",
            "Available Schemas:",
        ]

        for schema in schemas:
            lines.append(f"  - {schema}")

        lines.append("")
        lines.append("Available Tables:")

        # Group tables by schema
        schema_tables: dict[str, list] = {}
        for table in tables:
            schema_name = table.get("schema", "dbo")
            if schema_name not in schema_tables:
                schema_tables[schema_name] = []
            schema_tables[schema_name].append(table.get("name"))

        for schema, table_list in schema_tables.items():
            lines.append(f"  Schema: {schema}")
            for table_name in sorted(table_list):
                lines.append(f"    - {table_name}")

        return "\n".join(lines)


# ============================================================================
# SQL Validator Executor
# ============================================================================


class SQLValidatorExecutor(Executor):
    """Validates SQL queries for safety and correctness."""

    DANGEROUS_KEYWORDS = [
        "DROP",
        "TRUNCATE",
        "DELETE",
        "ALTER",
        "CREATE",
        "INSERT",
        "UPDATE",
        "EXEC",
        "EXECUTE",
        "sp_",
        "xp_",
    ]

    def __init__(self, max_rows: int = 1000, allow_write: bool = False, id: str = "sql_validator"):
        super().__init__(id=id)
        self.max_rows = max_rows
        self.allow_write = allow_write

    @handler
    async def validate(self, messages: list[ChatMessage], ctx: WorkflowContext[list[ChatMessage]]) -> None:
        """Validate SQL query from LLM response."""
        logger.info("Validating SQL query")

        # Extract SQL from the last message (agent's response)
        last_message_text = messages[-1].text if messages else ""
        sql_query = self._extract_sql_from_message(last_message_text)

        if not sql_query:
            error_msg = ChatMessage(
                role=Role.SYSTEM,
                text="Error: No valid SQL query found in the response. Please regenerate.",
            )
            await ctx.send_message([error_msg])
            return

        # Perform safety checks
        is_safe, warnings = self._check_safety(sql_query)

        if not is_safe:
            error_msg = ChatMessage(
                role=Role.SYSTEM,
                text=f"SQL Safety Check Failed:\n{chr(10).join(warnings)}\n\nPlease generate a safer query.",
            )
            await ctx.send_message([error_msg])
            return

        # Add row limit if not present
        sql_query = self._ensure_row_limit(sql_query)

        # Validation passed
        logger.info(f"SQL validation passed: {sql_query[:100]}...")
        validated_msg = ChatMessage(
            role=Role.SYSTEM,
            text=f"VALIDATED SQL QUERY:\n```sql\n{sql_query}\n```\n\nQuery is safe to execute.",
        )
        await ctx.send_message([validated_msg])

    def _extract_sql_from_message(self, text: str) -> str | None:
        """Extract SQL query from LLM response."""
        # Look for SQL in code blocks
        sql_block_pattern = r"```sql\s*(.*?)\s*```"
        match = re.search(sql_block_pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()

        # Look for SELECT statements
        select_pattern = r"(SELECT\s+.*?;?)\s*$"
        match = re.search(select_pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()

        return None

    def _check_safety(self, sql: str) -> tuple[bool, list[str]]:
        """Check if SQL query is safe to execute."""
        warnings = []
        sql_upper = sql.upper()

        # Check for dangerous keywords
        for keyword in self.DANGEROUS_KEYWORDS:
            if keyword in sql_upper:
                if keyword in ["DELETE", "UPDATE", "INSERT"] and self.allow_write:
                    warnings.append(f"âš ï¸  Write operation detected: {keyword}")
                else:
                    warnings.append(f"âŒ Forbidden operation: {keyword}")
                    return False, warnings

        # Check for comments (potential SQL injection)
        if "--" in sql or "/*" in sql:
            warnings.append("âš ï¸  SQL comments detected - review for safety")

        # Check for multiple statements
        if sql.count(";") > 1:
            warnings.append("âŒ Multiple statements not allowed")
            return False, warnings

        return True, warnings

    def _ensure_row_limit(self, sql: str) -> str:
        """Add row limit to SELECT queries if not present."""
        sql_upper = sql.upper()

        if not sql_upper.strip().startswith("SELECT"):
            return sql

        # Check if TOP or LIMIT already exists
        if "TOP " in sql_upper or "LIMIT " in sql_upper:
            return sql

        # Add TOP clause for SQL Server
        sql = re.sub(r"^(SELECT)\s+", f"SELECT TOP {self.max_rows} ", sql, flags=re.IGNORECASE)

        return sql


# ============================================================================
# Query Executor Executor
# ============================================================================


class QueryExecutorExecutor(Executor):
    """Executes validated SQL queries against Azure SQL Database."""

    def __init__(self, use_real_db: bool = True, id: str = "query_executor"):
        super().__init__(id=id)
        self.use_real_db = use_real_db
        self.max_rows = int(os.environ.get("MAX_ROWS", "1000"))

    @handler
    async def execute_query(self, messages: list[ChatMessage], ctx: WorkflowContext[list[ChatMessage]]) -> None:
        """Execute SQL query and return results.
        
        If use_real_db=True, executes against actual database.
        Otherwise, returns mock results for demo purposes.
        """
        logger.info(f"Executing SQL query (real_db={self.use_real_db})")

        # Extract SQL from the last message (validator's output)
        last_message_text = messages[-1].text if messages else ""
        sql_query = self._extract_validated_sql(last_message_text)

        if not sql_query:
            error_msg = ChatMessage(
                role=Role.SYSTEM, text="Error: No validated SQL query found to execute."
            )
            await ctx.send_message([error_msg])
            return

        try:
            import time
            start_time = time.time()

            if self.use_real_db:
                result = await self._execute_real_query(sql_query)
            else:
                logger.warning("âš ï¸  Using mock query execution - set use_real_db=True for actual execution")
                result = self._execute_mock_query(sql_query)
            
            execution_time = (time.time() - start_time) * 1000

            # Format results
            results_text = self._format_results(result, execution_time)

            results_msg = ChatMessage(role=Role.SYSTEM, text=results_text)
            await ctx.send_message([results_msg])

        except Exception as e:
            logger.error(f"Query execution failed: {e}")
            error_msg = ChatMessage(
                role=Role.SYSTEM,
                text=f"Query Execution Error:\n{str(e)}\n\nPlease review and regenerate the query.",
            )
            await ctx.send_message([error_msg])
    
    async def _execute_real_query(self, sql: str) -> dict:
        """Execute query against real database.
        
        Args:
            sql: SQL query to execute
            
        Returns:
            Dictionary with query results
        """
        from db_utils import create_connection_from_env
        
        logger.info(f"Executing query against database: {sql[:100]}...")
        
        with create_connection_from_env() as db:
            result = db.execute_query(sql, max_rows=self.max_rows)
        
        logger.info(f"âœ… Query executed: {result['row_count']} rows, {result['execution_time_ms']:.2f}ms")
        
        # Convert to expected format
        return {
            "rowCount": result["row_count"],
            "columns": result["columns"],
            "rows": result["data"],
            "executionTimeMs": result["execution_time_ms"],
        }

    def _execute_mock_query(self, sql: str) -> dict:
        """Execute mock query for demonstration purposes.
        
        TODO: Replace with actual database query execution.
        """
        # Return mock data
        return {
            "rowCount": 3,
            "columns": [
                {"name": "CustomerName"},
                {"name": "TotalRevenue"},
            ],
            "rows": [
                {"CustomerName": "Acme Corp", "TotalRevenue": 125000.00},
                {"CustomerName": "Widget Inc", "TotalRevenue": 98500.50},
                {"CustomerName": "Global Services", "TotalRevenue": 87250.75},
            ]
        }

    def _extract_validated_sql(self, text: str) -> str | None:
        """Extract validated SQL from message."""
        sql_block_pattern = r"```sql\s*(.*?)\s*```"
        match = re.search(sql_block_pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()
        return None

    def _format_results(self, result: dict, execution_time: float) -> str:
        """Format query results for LLM interpretation with table formatting."""
        lines = ["QUERY RESULTS:", ""]

        # Add execution metadata
        lines.append(f"â±ï¸  Execution Time: {execution_time:.2f}ms")
        lines.append(f"ðŸ“Š Rows Returned: {result.get('rowCount', 0)}")
        lines.append("")

        # Add data rows in table format
        rows = result.get("rows", [])
        if rows:
            columns = result.get("columns", [])
            column_names = [col.get("name", "") for col in columns]
            
            # Calculate column widths (limit to reasonable max)
            col_widths = {}
            for col_name in column_names:
                max_width = len(col_name)
                for row in rows[:10]:  # Only check first 10 rows for width
                    val_str = str(row.get(col_name, ""))
                    max_width = max(max_width, len(val_str))
                col_widths[col_name] = min(max_width + 2, 50)  # Max 50 chars per column
            
            # Create table header
            header_parts = []
            separator_parts = []
            for col_name in column_names:
                width = col_widths[col_name]
                header_parts.append(col_name.ljust(width))
                separator_parts.append("â”€" * width)
            
            lines.append("â”‚ " + " â”‚ ".join(header_parts) + " â”‚")
            lines.append("â”œâ”€" + "â”€â”¼â”€".join(separator_parts) + "â”€â”¤")
            
            # Add data rows (show first 10)
            for row in rows[:10]:
                row_parts = []
                for col_name in column_names:
                    value = row.get(col_name, "")
                    # Truncate long values
                    val_str = str(value)
                    if len(val_str) > 47:
                        val_str = val_str[:47] + "..."
                    row_parts.append(val_str.ljust(col_widths[col_name]))
                lines.append("â”‚ " + " â”‚ ".join(row_parts) + " â”‚")
            
            # Add footer
            if len(rows) > 10:
                lines.append("â””â”€" + "â”€â”´â”€".join(separator_parts) + "â”€â”˜")
                lines.append(f"... and {len(rows) - 10} more rows")
            else:
                lines.append("â””â”€" + "â”€â”´â”€".join(separator_parts) + "â”€â”˜")
        else:
            lines.append("âŒ No rows returned.")

        lines.append("")
        lines.append("Please interpret these results and provide a natural language answer to the user's question.")

        return "\n".join(lines)


# ============================================================================
# Results Exporter Executor
# ============================================================================


class ResultsExporterExecutor(Executor):
    """Exports query results to CSV and Excel formats."""
    
    def __init__(self, export_dir: str = "exports", id: str = "results_exporter"):
        super().__init__(id=id)
        self.export_dir = export_dir
    
    @handler
    async def export_results(self, messages: list[ChatMessage], ctx: WorkflowContext[list[ChatMessage]]) -> None:
        """Extract query results from messages and export to CSV/Excel.
        
        Looks for QUERY RESULTS in the conversation and exports the data.
        """
        import csv
        from datetime import datetime
        from pathlib import Path
        
        logger.info("Checking for query results to export...")
        
        # Find the message with query results
        results_data = None
        user_question = None
        
        for msg in messages:
            # Extract user question
            if msg.role == Role.USER and not user_question:
                user_question = msg.text
            
            # Look for QUERY RESULTS in system messages
            if msg.role == Role.SYSTEM and "QUERY RESULTS:" in msg.text:
                results_data = self._extract_results_from_message(msg.text)
                break
        
        if not results_data or not results_data.get("rows"):
            logger.info("No data to export (query returned no rows)")
            return
        
        # Create exports directory
        export_path = Path(self.export_dir)
        export_path.mkdir(exist_ok=True)
        
        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"query_results_{timestamp}"
        
        # Export to CSV
        csv_file = export_path / f"{base_filename}.csv"
        self._export_to_csv(results_data, csv_file, user_question)
        
        # Export to Excel (if openpyxl is available)
        try:
            excel_file = export_path / f"{base_filename}.xlsx"
            self._export_to_excel(results_data, excel_file, user_question)
            
            logger.info(f"âœ… Results exported to CSV and Excel: {base_filename}")
            
            # Send confirmation message
            export_msg = ChatMessage(
                role=Role.SYSTEM,
                text=f"""ðŸ“Š **Export Complete**

Results have been saved to:
- CSV: `{csv_file}`
- Excel: `{excel_file}`

Total rows exported: {len(results_data['rows'])}""",
            )
            await ctx.send_message([export_msg])
            
        except ImportError:
            logger.warning("openpyxl not installed - Excel export skipped")
            
            # Send CSV-only confirmation
            export_msg = ChatMessage(
                role=Role.SYSTEM,
                text=f"""ðŸ“Š **Export Complete**

Results have been saved to:
- CSV: `{csv_file}`

Total rows exported: {len(results_data['rows'])}

ðŸ’¡ Tip: Install openpyxl for Excel export: `pip install openpyxl`""",
            )
            await ctx.send_message([export_msg])
    
    def _extract_results_from_message(self, text: str) -> dict | None:
        """Extract structured results data from formatted message text.
        
        Parses the table format to extract column names and row data.
        """
        lines = text.split("\n")
        
        # Find the table header line (starts with â”‚)
        header_line = None
        data_start_idx = None
        
        for i, line in enumerate(lines):
            if line.strip().startswith("â”‚") and "â”‚" in line[1:]:
                # Check if this is the header (not the separator line with â”€)
                if "â”€" not in line:
                    header_line = line
                    # Data starts after the separator line
                    data_start_idx = i + 2
                    break
        
        if not header_line or data_start_idx is None:
            return None
        
        # Parse column names from header
        column_names = [col.strip() for col in header_line.split("â”‚")[1:-1]]
        
        # Parse data rows
        rows = []
        for line in lines[data_start_idx:]:
            if line.strip().startswith("â”‚") and "â”€" not in line and "â””" not in line:
                values = [val.strip() for val in line.split("â”‚")[1:-1]]
                if len(values) == len(column_names):
                    row_dict = dict(zip(column_names, values))
                    rows.append(row_dict)
            elif "â””" in line:
                break  # End of table
        
        return {
            "columns": [{"name": col} for col in column_names],
            "rows": rows,
        }
    
    def _export_to_csv(self, results_data: dict, filepath: Path, user_question: str | None = None):
        """Export results to CSV file."""
        import csv
        
        columns = [col["name"] for col in results_data["columns"]]
        rows = results_data["rows"]
        
        with open(filepath, "w", newline="", encoding="utf-8") as f:
            # Add user question as comment if available
            if user_question:
                f.write(f"# Question: {user_question}\n")
                f.write(f"# Generated: {filepath.stem}\n")
                f.write("#\n")
            
            writer = csv.DictWriter(f, fieldnames=columns)
            writer.writeheader()
            writer.writerows(rows)
        
        logger.info(f"CSV export complete: {filepath}")
    
    def _export_to_excel(self, results_data: dict, filepath: Path, user_question: str | None = None):
        """Export results to Excel file with formatting."""
        try:
            from openpyxl import Workbook
            from openpyxl.styles import Font, PatternFill, Alignment
            from openpyxl.utils import get_column_letter
        except ImportError:
            raise ImportError("openpyxl is required for Excel export. Install with: pip install openpyxl")
        
        columns = [col["name"] for col in results_data["columns"]]
        rows = results_data["rows"]
        
        wb = Workbook()
        ws = wb.active
        ws.title = "Query Results"
        
        # Add user question at the top if available
        current_row = 1
        if user_question:
            ws.cell(row=current_row, column=1, value="User Question:")
            ws.cell(row=current_row, column=1).font = Font(bold=True)
            current_row += 1
            
            ws.cell(row=current_row, column=1, value=user_question)
            ws.merge_cells(start_row=current_row, start_column=1, end_row=current_row, end_column=len(columns))
            current_row += 2  # Add blank row
        
        # Write headers with formatting
        header_row = current_row
        for col_idx, col_name in enumerate(columns, 1):
            cell = ws.cell(row=header_row, column=col_idx, value=col_name)
            cell.font = Font(bold=True, color="FFFFFF")
            cell.fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
            cell.alignment = Alignment(horizontal="center", vertical="center")
        
        # Write data rows
        for row_idx, row_data in enumerate(rows, header_row + 1):
            for col_idx, col_name in enumerate(columns, 1):
                value = row_data.get(col_name, "")
                ws.cell(row=row_idx, column=col_idx, value=value)
        
        # Auto-size columns
        for col_idx, col_name in enumerate(columns, 1):
            max_length = len(col_name)
            for row_data in rows[:100]:  # Check first 100 rows for width
                value_str = str(row_data.get(col_name, ""))
                max_length = max(max_length, len(value_str))
            
            # Set column width (limit to reasonable max)
            ws.column_dimensions[get_column_letter(col_idx)].width = min(max_length + 2, 50)
        
        # Freeze header row
        ws.freeze_panes = ws.cell(row=header_row + 1, column=1)
        
        wb.save(filepath)
        logger.info(f"Excel export complete: {filepath}")
